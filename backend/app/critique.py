import json
import os
import re
from datetime import datetime
from typing import Any, Dict, List, Optional
from pathlib import Path

from app.config import LLMClient, GROQ_MODEL
from app.qa import answer_question

PROMPT_ISSUE_TAGS = [
    "missing_context",
    "too_vague",
    "no_format_specified",
    "length_unspecified",
    "ambiguous_audience",
    "multi_question",
]

QUALITY_STOP_THRESHOLD = 0.95
MAX_HALLUCINATION_FOR_EARLY_STOP = 0.05

CRITIQUE_LOG_PATH = Path(
    os.getenv("CRITIQUE_LOG_PATH", "data/critique_log.jsonl")
)


def _safe_str(value: Any) -> str:
    if value is None:
        return ""
    if isinstance(value, str):
        return value
    try:
        return str(value)
    except Exception:
        return ""


def _safe_tags(parsed: Dict[str, Any]) -> List[str]:
    raw = parsed.get("prompt_issue_tags") or parsed.get("issue_tags") or []
    if not isinstance(raw, list):
        return []
    out: List[str] = []
    for t in raw:
        if isinstance(t, str) and t in PROMPT_ISSUE_TAGS:
            out.append(t)
    return out


def _build_critique_prompt(
    question: str, answer: str, context: List[str]
) -> str:
    ctx = "\n\n---\n\n".join(context)
    tags_str = ", ".join(f'"{t}"' for t in PROMPT_ISSUE_TAGS)

    return f"""
You are a meticulous AI assistant that critiques another model's answer AND the user's question.

You are given:
- the original user question ("prompt")
- an answer generated by a base model
- supporting context passages from a knowledge base

Your job:
1. Critique the *answer* (correctness, completeness, clarity, hallucinations).
2. Critique the *prompt* (how its phrasing may have limited the answer).
3. Propose a better version of the prompt.
4. Tag common prompt issues using a small set of machine-readable tags.

Context:
{ctx}

Prompt:
{question}

Answer:
{answer}

Return a SINGLE JSON object with the following EXACT structure:

{{
  "answer_critique_markdown": "markdown text explaining quality of the answer...",
  "prompt_feedback_markdown": "markdown text explaining how to improve the prompt...",
  "improved_prompt": "a rewritten version of the prompt that should yield a better answer...",
  "prompt_issue_tags": ["missing_context", "no_format_specified"],
  "scores": {{
    "correctness": 0.0,
    "completeness": 0.0,
    "clarity": 0.0,
    "hallucination_risk": 0.0
  }}
}}

Rules:
- Use markdown in the *_markdown fields (bullet points, bold, etc.).
- "improved_prompt" MUST stay faithful to the user's original intent.
- "prompt_issue_tags" must be a subset of: [{tags_str}].
- "scores" values are floats between 0 and 1.
- VERY IMPORTANT: The JSON MUST be valid.
  - All keys and string values MUST use double quotes as in standard JSON.
  - Inside any string value, do NOT include raw double quotes. If you need to
    mention a title or phrase, either:
      - use single quotes inside the string, or
      - omit the quotes entirely.
  - Do NOT include trailing commas.
- Output *only* raw JSON, with no extra commentary before or after it.
"""


def _safe_json_parse(raw: str) -> Dict[str, Any] | None:
    raw = (raw or "").strip()
    if not raw:
        return None

    try:
        return json.loads(raw)
    except Exception:
        pass

    fenced = re.search(
        r"```(?:json)?\s*(\{.*?\})\s*```",
        raw,
        re.DOTALL | re.IGNORECASE,
    )
    if fenced:
        block = fenced.group(1).strip()
        try:
            return json.loads(block)
        except Exception:
            pass

    brace_match = re.search(r"\{.*\}", raw, re.DOTALL)
    if brace_match:
        block = brace_match.group(0)
        try:
            return json.loads(block)
        except Exception:
            pass

    return None


def _safe_scores(data: Dict[str, Any]) -> Dict[str, Optional[float]]:
    scores = data.get("scores") or {}

    def _num(key: str) -> Optional[float]:
        try:
            v = scores.get(key, None)
            if v is None:
                return None
            return float(v)
        except Exception:
            return None

    return {
        "correctness": _num("correctness"),
        "completeness": _num("completeness"),
        "clarity": _num("clarity"),
        "hallucination_risk": _num("hallucination_risk"),
    }


def reset_critique_log_file() -> None:
    """
    Delete the critique log file (if it exists).
    Used by the /reset-critique-log endpoint.
    """
    if CRITIQUE_LOG_PATH.exists():
        CRITIQUE_LOG_PATH.unlink()


def run_critique(
    question: str,
    answer_model: str,
    critic_model: Optional[str] = None,
    top_k: int = 5,
    doc_name: Optional[str] = None,
    self_correct: bool = False,
    similarity: Optional[str] = None,
) -> Dict[str, Any]:
    """
    1) Use the QA pipeline with `answer_model` to answer the question.
    2) Use `critic_model` (or default GROQ_MODEL) to critique the answer + prompt.
    3) If self_correct=True, run up to two critique-&-repair rounds.
    """

    llm = LLMClient()
    critic = critic_model or GROQ_MODEL

    max_rounds = 2 if self_correct else 1

    rounds: List[Dict[str, Any]] = []

    current_question = question
    final_answer = ""
    final_context: List[str] = []
    final_sources: List[Dict[str, Any]] = []
    final_answer_critique = ""
    final_prompt_feedback = ""
    final_improved_prompt = ""
    final_tags: List[str] = []
    final_scores: Dict[str, Optional[float]] | None = None

    for round_idx in range(1, max_rounds + 1):
        answer, context, sources = answer_question(
            current_question,
            k=top_k,
            doc_name=doc_name,
            model=answer_model,
            similarity=similarity,
        )

        prompt = _build_critique_prompt(current_question, answer, context)
        raw = llm.complete(prompt, model=critic)

        parsed = _safe_json_parse(raw) or {}
        answer_critique = _safe_str(parsed.get(
            "answer_critique_markdown") or parsed.get("answer_feedback_markdown"))
        prompt_feedback = _safe_str(parsed.get("prompt_feedback_markdown"))
        improved_prompt = _safe_str(parsed.get("improved_prompt"))
        tags = _safe_tags(parsed)
        scores = _safe_scores(parsed)

        round_entry = {
            "round": round_idx,
            "question": current_question,
            "answer": answer,
            "context": context,
            "sources": sources,
            "answer_critique_markdown": answer_critique,
            "prompt_feedback_markdown": prompt_feedback,
            "improved_prompt": improved_prompt,
            "prompt_issue_tags": tags,
            "scores": scores,
        }
        rounds.append(round_entry)

        final_answer = answer
        final_context = context
        final_sources = sources
        final_answer_critique = answer_critique
        final_prompt_feedback = prompt_feedback
        final_improved_prompt = improved_prompt
        final_tags = tags
        final_scores = scores

        if round_idx == max_rounds:
            break

        if not improved_prompt:
            break

        correctness = scores.get("correctness") if scores else None
        hallucination = scores.get("hallucination_risk") if scores else None

        if (
            correctness is not None
            and correctness >= QUALITY_STOP_THRESHOLD
            and (hallucination is None or hallucination <= MAX_HALLUCINATION_FOR_EARLY_STOP)
        ):
            break
        current_question = improved_prompt

    try:
        log_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "question": question,
            "answer_model": answer_model,
            "critic_model": critic,
            "doc_name": doc_name,
            "self_correct": self_correct,
            "similarity": similarity,
            "rounds": rounds,
        }
        os.makedirs("data", exist_ok=True)
        with open("data/critique_log.jsonl", "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
    except Exception as e:
        print("Failed to write critique_log.jsonl:", e)

    return {
        "question": question,
        "answer_model": answer_model,
        "critic_model": critic,
        "answer": final_answer,
        "context": final_context,
        "sources": final_sources,
        "answer_critique_markdown": final_answer_critique,
        "prompt_feedback_markdown": final_prompt_feedback,
        "improved_prompt": final_improved_prompt,
        "prompt_issue_tags": final_tags,
        "scores": final_scores,
        "rounds": rounds,
    }
