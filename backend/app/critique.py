# backend/app/critique.py
import json
import os
from datetime import datetime
from typing import Any, Dict, List, Optional

from app.config import LLMClient, GROQ_MODEL
from app.qa import answer_question

PROMPT_ISSUE_TAGS = [
    "missing_context",
    "too_vague",
    "no_format_specified",
    "length_unspecified",
    "ambiguous_audience",
    "multi_question",
]


def _build_critique_prompt(
    question: str, answer: str, context: List[str]
) -> str:
    ctx = "\n\n---\n\n".join(context)
    tags_str = ", ".join(f'"{t}"' for t in PROMPT_ISSUE_TAGS)

    return f"""
You are a meticulous AI assistant that critiques another model's answer AND the user's question.

You are given:
- the original user question ("prompt")
- an answer generated by a base model
- supporting context passages from a knowledge base

Your job:
1. Critique the *answer* (correctness, completeness, clarity, hallucinations).
2. Critique the *prompt* (how its phrasing may have limited the answer).
3. Propose a better version of the prompt.
4. Tag common prompt issues using a small set of machine-readable tags.

Context:
{ctx}

Prompt:
{question}

Answer:
{answer}

Return a SINGLE JSON object with the following EXACT structure:

{{
  "answer_critique_markdown": "markdown text explaining quality of the answer...",
  "prompt_feedback_markdown": "markdown text explaining how to improve the prompt...",
  "improved_prompt": "a rewritten version of the prompt that should yield a better answer...",
  "prompt_issue_tags": ["missing_context", "no_format_specified"],
  "scores": {{
    "correctness": 0.0,
    "completeness": 0.0,
    "clarity": 0.0,
    "hallucination_risk": 0.0
  }}
}}

Rules:
- Use markdown in the *_markdown fields (bullet points, bold, etc.).
- "improved_prompt" MUST stay faithful to the user's original intent.
- "prompt_issue_tags" must be a subset of: [{tags_str}].
- "scores" values are floats between 0 and 1.
- VERY IMPORTANT: The JSON MUST be valid.
  - All keys and string values MUST use double quotes as in standard JSON.
  - Inside any string value, do NOT include raw double quotes. If you need to
    mention a title or phrase, either:
      - use single quotes inside the string, or
      - omit the quotes entirely.
  - Do NOT include trailing commas.
- Output *only* raw JSON, with no extra commentary before or after it.
"""


def _safe_list_str(value: Any) -> List[str]:
    if isinstance(value, list):
        return [str(v) for v in value if isinstance(v, (str, int, float))]
    if isinstance(value, str):
        return [value]
    return []


def _safe_scores(data: Dict[str, Any]) -> Dict[str, Optional[float]]:
    scores = data.get("scores") or {}

    def _num(key: str) -> Optional[float]:
        try:
            v = scores.get(key, None)
            if v is None:
                return None
            return float(v)
        except Exception:
            return None

    return {
        "correctness": _num("correctness"),
        "completeness": _num("completeness"),
        "clarity": _num("clarity"),
        "hallucination_risk": _num("hallucination_risk"),
    }


def run_critique(
    question: str,
    answer_model: str,
    critic_model: Optional[str] = None,
    top_k: int = 5,
    doc_name: Optional[str] = None,
) -> Dict[str, Any]:
    """
    1) Use the QA pipeline with `answer_model` to answer the question.
    2) Use `critic_model` (or default GROQ_MODEL) to critique the answer + prompt.
    3) Return a dict ready for CritiqueResponse.
    """

    # Step 1: get answer from base QA
    answer, context, sources = answer_question(
        question,
        k=top_k,
        doc_name=doc_name,
        model=answer_model,
    )

    # Step 2: run critic model with strict-ish JSON prompt
    llm = LLMClient()
    critic = critic_model or GROQ_MODEL
    prompt = _build_critique_prompt(question, answer, context)
    raw = llm.complete(prompt, model=critic)

    parsed: Dict[str, Any]
    try:
        parsed = json.loads(raw)
    except Exception as e:
        # Debug logs so you can see exactly what went wrong
        print("Failed to parse critique JSON:", e)
        print("Raw critique output:")
        print(raw)

        # Fallback: treat raw as answer critique only
        parsed = {
            "answer_critique_markdown": str(raw),
            "prompt_feedback_markdown": "The critic returned non-JSON output; treat the text above as the answer critique.",
            "improved_prompt": question,
            "prompt_issue_tags": [],
            "scores": {},
        }

    answer_critique = str(parsed.get("answer_critique_markdown", "") or "")
    prompt_feedback = str(parsed.get("prompt_feedback_markdown", "") or "")
    improved_prompt = str(parsed.get("improved_prompt", "") or question)
    tags = _safe_list_str(parsed.get("prompt_issue_tags", []))
    scores = _safe_scores(parsed)

    # Step 3: log to JSONL for later analysis
    os.makedirs("data", exist_ok=True)
    log_entry = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "question": question,
        "answer_model": answer_model,
        "critic_model": critic,
        "answer": answer,
        "context": context,
        "prompt_feedback_markdown": prompt_feedback,
        "answer_critique_markdown": answer_critique,
        "improved_prompt": improved_prompt,
        "prompt_issue_tags": tags,
        "scores": scores,
    }
    try:
        with open("data/critique_log.jsonl", "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
    except Exception as e:
        print("Failed to write critique_log.jsonl:", e)

    return {
        "question": question,
        "answer_model": answer_model,
        "critic_model": critic,
        "answer": answer,
        "context": context,
        "sources": sources,
        "answer_critique_markdown": answer_critique,
        "prompt_feedback_markdown": prompt_feedback,
        "improved_prompt": improved_prompt,
        "prompt_issue_tags": tags,
        "scores": scores,
    }
